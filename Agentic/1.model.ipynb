{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2982a607",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda21482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser, CommaSeparatedListOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "from langchain.chains import ConversationChain, LLMChain, SequentialChain\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fce78f",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae8bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''     Env setup and Gemini model initialization     '''\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is loaded\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    print(\"Error: GOOGLE_API_KEY not found in environment variables.\")\n",
    "    exit()\n",
    "\n",
    "# models: ['gemini-2.5-pro', 'gemma-3-27b-it', 'gemini-2.5-flash-lite]\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\", \n",
    "    temperature=0.7, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa75a4",
   "metadata": {},
   "source": [
    "#### Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e8dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm.invoke(\n",
    "  [\n",
    "      SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "      HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "      AIMessage(content=\"You should try a CrossFit class\"),\n",
    "      HumanMessage(content=\"How often should I attend?\")\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad1a6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You should aim for 3-5 CrossFit sessions per week, allowing for rest days.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--6168f114-2eee-4018-9cd5-47b4966ffe6e-0' usage_metadata={'input_tokens': 44, 'output_tokens': 18, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6e531",
   "metadata": {},
   "source": [
    "#### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a7659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about cats')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"    String prompt templates    \"\"\"\n",
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\": \"funny\", \"topic\": \"cats\"}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bec722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"    Chat prompt templates    \"\"\"\n",
    "\n",
    "# Create a ChatPromptTemplate with a list of message tuples\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    " (\"system\", \"You are a helpful assistant\"),\n",
    " (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "# Create a dictionary with the variable to be inserted into the template      \n",
    "input_ = {\"topic\": \"cats\"}\n",
    "\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc551d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"    MessagePlaceholder    \"\"\"\n",
    "\n",
    "# Create a ChatPromptTemplate with a system message and a placeholder for multiple messages\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "(\"system\", \"You are a helpful assistant\"),\n",
    "MessagesPlaceholder(\"msgs\")  # This will be replaced with one or more messages\n",
    "])\n",
    "\n",
    "# Create an input dictionary where the key matches the MessagesPlaceholder name\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416a03c",
   "metadata": {},
   "source": [
    "#### Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3642f54",
   "metadata": {},
   "source": [
    "\"\"\"    JSON    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c5a9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure (JSON) (LLM output).\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92581a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why do programmers prefer dark mode?',\n",
       " 'punchline': 'Because light attracts bugs.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a less common developer joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Get the formatting instructions for the output parser\n",
    "# This generates guidance text that tells the LLM how to format its response\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt template that includes:\n",
    "# 1. Instructions for the LLM to answer the user's query\n",
    "# 2. Format instructions to ensure the LLM returns properly structured data\n",
    "# 3. The actual user query placeholder\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],  # Dynamic variables that will be provided when invoking the chain\n",
    "    partial_variables={\"format_instructions\": format_instructions},  # Static variables set once when creating the prompt\n",
    ")\n",
    "\n",
    "# Create a processing chain that:\n",
    "# 1. Formats the prompt using the template\n",
    "# 2. Sends the formatted prompt to the Llama LLM\n",
    "# 3. Parses the LLM's response using the output parser to extract structured data\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain with a specific query about jokes\n",
    "# This will:\n",
    "# 1. Format the prompt with the joke query\n",
    "# 2. Send it to the LLM\n",
    "# 3. Parse the response into the structure defined by your output parser\n",
    "# 4. Return the structured result\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d4eb1",
   "metadata": {},
   "source": [
    "\"\"\"    CSV    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6fd097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vanilla',\n",
       " 'Chocolate',\n",
       " 'Strawberry',\n",
       " 'Mint Chocolate Chip',\n",
       " 'Cookies and Cream']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the parser that will convert comma-separated text into a Python list\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# These instructions explain to the LLM that it should return items in a comma-separated format\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt template that:\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\nList five {subject}.\",\n",
    "    input_variables=[\"subject\"],  # This variable will be provided when the chain is invoked\n",
    "    partial_variables={\"format_instructions\": format_instructions},  # This variable is set once when creating the prompt\n",
    ")\n",
    "\n",
    "# Build a processing chain that:\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the processing chain with \"ice cream flavors\" as the subject\n",
    "# This will:\n",
    "# 1. Substitute \"ice cream flavors\" into the prompt template\n",
    "# 2. Send the formatted prompt to the LLM\n",
    "# 3. Parse the LLM's comma-separated response into a Python list\n",
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a072fb7",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751e62d",
   "metadata": {},
   "source": [
    "`Document Object `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e2dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Document instance with:\n",
    "# 1. page_content: The actual text content about Python\n",
    "# 2. metadata: A dictionary containing additional information about this document\n",
    "Document(\n",
    "    page_content=\"\"\"Python is an interpreted high-level general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "    metadata={\n",
    "        'my_document_id' : 234234,                      # Unique identifier for this document\n",
    "        'my_document_source' : \"About Python\",          # Source or title information\n",
    "        'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1ca37",
   "metadata": {},
   "source": [
    "`Document Loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c04e523c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content=\"* corresponding author - jkim72@kent.edu \\nRevolutionizing Mental Health Care through \\nLangChain: A Journey with a Large Language \\nModel\\nAditi Singh \\n Computer Science  \\n Cleveland State University  \\n a.singh22@csuohio.edu \\nAbul Ehtesham  \\nThe Davey Tree Expert \\nCompany  \\nabul.ehtesham@davey.com \\nSaifuddin Mahmud  \\nComputer Science & \\nInformation Systems  \\n Bradley University  \\nsmahmud@bradley.edu  \\nJong-Hoon Kim* \\n Computer Science,  \\nKent State University,  \\njkim72@kent.edu \\nAbstract‚Äî Mental health challenges are on the rise in our \\nmodern society, and the imperative to address mental disorders, \\nespecially regarding anxiety, depression, and suicidal thoughts, \\nunderscores the need for effective interventions. This paper \\ndelves into the application of recent advancements in pretrained \\ncontextualized language models to introduce MindGuide, an \\ninnovative chatbot serving as a mental health assistant for \\nindividuals seeking guidance and support in these critical areas. \\nMindGuide lever ages the capabilities of LangChain and its \\nChatModels, specifically Chat OpenAI, as the bedrock of its \\nreasoning engine. The system incorporates key features such as \\nLangChain's ChatPrompt Template, HumanMessage  Prompt \\nTemplate, ConversationBufferMemory, and LLMChain, \\ncreating an advanced solution for early detection and \\ncomprehensive support within the field of mental health. \\nAdditionally, the paper discusses the implementation of \\nStreamlit to enhance the user ex perience and interaction with \\nthe chatbot. Th is novel approach holds great promise for \\nproactive mental health intervention and assistance. \\nKeywords ‚ÄîLarge Language models , LangChain, Chatbot, \\nPretrained models, Mental health, Mental health support. \\nI. INTRODUCTION \\nThe issue of mental health is an international situation, \\naffecting people in each particularly developed nations and \\nemerging markets. According to the World Health \\nOrganization's Mental Health Action Plan (2013-2020), it's far \\nestimated that around one in four humans international face  \\nnumerous kinds of mental disorders. This statistic underscores \\nthe vast nature of mental health demanding situations \\nthroughout extraordinary demographic businesses and areas. \\nHowever, what makes this situation even extra complex is \\nthe concerning truth that three out of each four people dealing \\nwith severe intellectual disorders do no longer have get entry \\nto the necessary remedy they require. This remedy gap \\nintensifies the weight of intellectual health troubles, leaving a \\nsizable part of the populace without the assist and care needed \\nto efficiently address their intellectual health issues. \\nFurthermore, periods like the recent global pandemic, the \\neffect of mental health issues becomes even more said. The \\nCOVID-19 pandemic, in particular, has highlighted how \\npublic health crises can extensively have an effect on mental \\nproperly-being. During such hard instances, a widespread part \\nof the population faces extended problems in having access to \\nmental fitness professionals. This emphasizes the urgent want \\nfor progressed intellectual health offerings and support \\nstructures. It underscores the urgency of addressing the mental \\nhealth disaster and developing complete answers to make \\ncertain that people global have the means to successfully deal \\nwith their mental fitness challenges.. \\nIn studies [1], it's pretty clear that there's a deep connection \\nbetween mental troubles and the chances of someone taking \\ntheir own life. And when you look at the big picture, it's quite \\nshocking - nearly a million people across the globe end their \\nlives every year, especially the young ones, making it the \\nsecond biggest reason for their passing . It's intriguing that \\nwhen someone attempts suicide, they often grapple with \\nmental challenges. It's like shifting from struggling with \\ndifficult thoughts to considering ending everything. This shift \\nis observable in how people express themselves and \\ninteract[2]. \\nOne practical approach to addressing mental illness and \\npreventing suicidal ideation is early identification. Recent \\nadvancements in deep learning have facilitated the \\ndevelopment of effective early detection methods [3]. A \\nnotable trend in natural language processing (NLP)  involves \\nthe use of contextualized pretrained language models  [4], \\nwhich have garnered substantial attention for their \\neffectiveness in various text processing tasks. \\nThis paper delves into the application of these recent \\nadvancements in pretrained contextualized large language \\nmodels to introduce MindGuide, an innovative chatbot \\ndesigned to function as a mental health assistant for \\nindividuals in need of guidance and support in these critical \\nareas. MindGuide relies on the capabilities of LangChain and \\nits ChatModels  [5], specifically Chat OpenAI [6], as the \\nfoundation of its reasoning engine. The system incorporates \\nkey components such as LangChain‚Äôs ChatPrompt Template \\n[7], HumanMessage , PromptTemplate, ConversationBuffer \\nMemory, and LLMChain [8], creating an advanced solution \\nfor early detection and comprehensive support within the field \\nof mental health. Additionally, the paper discusses the \\nimplementation of Streamlit to enhance the user experience \\nand interaction with the chatbot. \\nThe remainder of the paper is arranged accordingly. In \\nSection II, LangChain and its important components  are \\nintroduced. T he proposed methodology for developing the \\nwhole architecture is described  in Section III . Section IV \\nprovides an overview of Streamlit. Section V provides an \\nillustration of sequential interaction of MindGuide chatbot \\nand human. The conclusion is drawn in Section V. \\nII. LANGCHAIN \\nLangChain, with its open -source essence, emerges as a \\npromising solution, aiming to simplify the complex process of \\ndeveloping applications powered by large language models \\n(LLMs). This framework though the rapid delivery of building \\nblocks and pre-built chains for building large language model \\napplications shows the easy way developers can do it.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='LangChain helps us to unlock the ability to harness the \\nLLM‚Äôs immense potential in tasks such as document analysis, \\nchatbot development, code analysis, and countless other \\napplications. Whether your desire is to unlock deeper natural \\nlanguage understanding , enhance data, or circumvent \\nlanguage barriers through translation, LangChain is ready to \\nprovide the tools and programming support you need to do \\nwithout it that it is not only difficult but also fresh for you. Its \\ncore functionalities encompass: \\n1. Context-Aware Capabilities: LangChain facilitates the \\ndevelopment of applications that are inherently \\ncontext-aware. This means that these applications can \\nconnect to a language model and draw from various \\nsources of context, such as prompt instructions, a few-\\nshot examples, or existing content, to ground their \\nresponses effectively. \\n2. Reasoning Abilities: LangChain equips applications \\nwith the capacity to reason effectively. By relying on a \\nlanguage model, these applications can make informed \\ndecisions about how to respond based on the provided \\ncontext and determine the appropriate actions to take. \\nLangChain offers several key value propositions: \\nModular Components: It provides abstractions that \\nsimplify working with language models, along with a \\ncomprehensive collection of implementations for each \\nabstraction. These components are designed to be modular \\nand user -friendly, making them useful whethe r you are \\nutilizing the entire LangChain framework or not. \\nOff-the-Shelf Chains: LangChain offers pre -configured \\nchains, which are structured assemblies of components \\ntailored to accomplish specific high -level tasks. These pre -\\ndefined chains streamline the initial setup process and serve as \\nan ideal starting point for your projects. The MindGuide Bot \\nuses below components from LangChain. \\nA. ChatModel \\nWithin LangChain, a ChatModel is a specific kind of \\nlanguage model crafted to manage conversational \\ninteractions. Unlike traditional language models that take one \\nstring as input and generate a single string as output, \\nChatModels operate with a list of mes sages as input, \\ngenerating a message as output. \\nEach message in the list has two parts: the content and the \\nrole. The content is the actual text or substance of the message, \\nwhile the role denotes the role or source of the message (such \\nas \"User,\" \"Assistant,\" \"System,\" etc.). \\nThis approach with ChatModels opens the door to more \\ndynamic and interactive conversations with the language \\nmodel. It empowers the creation of chatbot applications, \\ncustomer support systems, or any other application involving \\nmulti-turn conversations. We utilized the ChatOpenAI \\nChatModel to create MindGuide chatbots specifically \\ndesigned to function as mental health therapists. In our \\ninteraction with OpenAI, we opted for an OpenAI API key to \\nengage with the ChatGpt3 turbo model and utilized a \\ntemperature value of 0.5. The steps to create an OpenAI API \\nkey are outlined [9].  \\nB. Message \\nIn the context of LangChain, messages  [10] refer to a list of \\nmessages that are used as input when interacting with a \\nChatModel. Each message in the list represents a specific turn \\nor exchange in a conversation. Each message in the messages \\nlist typically consists of two components: \\n‚Ä¢ content: This represents the actual text or content of \\nthe message. It can be a user query, a system \\ninstruction, or any other relevant information. \\n‚Ä¢ role: This represents the role or source of the \\nmessage. It defines who is speaking or generating \\nthe message. Common roles include \"User\", \\n\"Assistant\", \"System\", or any other custom role you \\ndefine. \\nThe chat model interface is based around messages rather \\nthan raw text. The types of messages supported in LangChain \\nare SystenMessage, HumanMessage, and AIMessage. \\nSystemMessage is the ChatMessage coming from the system \\nin its LangChain template  as illustrated in Figure 1. Human \\nMessage is a  ChatMessage coming from a human/user.  \\nAIMessage is a ChatMessage coming from an AI/assistant as \\nillustrated in Figure 2.  \\n \\n                   Figure 1. A System Message illustration  \\nYou are a compassionate and experienced mental \\nhealth therapist with a proven track record of \\nhelping patients overcome anxiety and other mental \\nhealth challenges. Your primary objective is to \\nsupport the patient in addressing their concerns \\nand guiding them towards positive change. In this \\ninteractive therapy session, you will engage with \\nthe patient by asking open -ended questions, \\nactively listening to their responses, and providing \\nempathetic feedback. Your approach is \\ncollaborative, and you strive to cr eate a safe and \\nnon-judgmental space for the patient to share their \\nthoughts and feelings. \\nAs the patient shares their struggles, you will \\nprovide insightful guidance and evidence -based \\nstrategies tailored to their unique needs. You may \\nalso offer practical exercises or resources to help \\nthem manage their symptoms and improve their \\nmental wellbeing. When necessary, you will gently \\nredirect the conversation back to the patient\\'s \\nprimary concerns related to anxiety, mental health, \\nor family issues. This ensures that each session is \\nproductive and focused on addressing the most \\npressing issues. Thro ughout the session, you \\nremain mindful of the patient\\'s emotional state and \\nadjust your approach accordingly. \\nYou recognize that everyone\\'s journey is \\ndifferent, and that progress can be incremental.  \\nBy building trust and fostering a strong \\ntherapeutic relationship, you empower the patient \\nto take ownership of their growth and development. \\nAt the end of the session, you will summarize key \\npoints from your discussion, highlighting the \\npatient\\'s strengths and areas for improvement. \\nTogether, you will set achievable goals for future \\nsessions, reinforcing a sense of hope and \\nmotivation. Your ultimate goal is to equip the \\npatient with the tools and skills needed to navigate \\nlife\\'s challenges with confidence and resilience.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Figure 2. An AIMessage illustration \\nC. Prompt Template \\nPrompt templates [10] allow you to structure input for LLMs. \\nThey provide a convenient way to format user inputs and \\nprovide instructions to generate responses. Prompt templates \\nhelp ensure that the LLM understands the desired context and \\nproduces relevant outputs. \\nThe prompt template classes in LangChain are built to \\nmake constructing prompts with dynamic inputs easier. Of \\nthese classes, the simplest is the PromptTemplate. \\nD. Chain \\nChains [11] in LangChain refer to the combination of \\nmultiple components to achieve specific tasks. They provide \\na structured and modular approach to building language \\nmodel applications. By combining different components, you \\ncan create chains that address various u se cases and \\nrequirements. Here are some advantages of using chains: \\n‚Ä¢ Modularity: Chains allow you to break down \\ncomplex tasks into smaller, manageable \\ncomponents. Each component can be developed and \\ntested independently, making it easier to maintain \\nand update the application. \\n‚Ä¢ Simplification: By combining components into a \\nchain, you can simplify the overall implementation \\nof your application. Chains abstract away the \\ncomplexity of working with individual components, \\nproviding a higher-level interface for developers. \\n‚Ä¢ Debugging: When an issue arises in your \\napplication, chains can help pinpoint the \\nproblematic component. By isolating the chain and \\ntesting each component individually, you can \\nidentify and troubleshoot any errors or unexpected \\nbehavior. \\n‚Ä¢ Maintenance: Chains make it easier to update or \\nreplace specific components without affecting the \\nentire application. If a new version of a component \\nbecomes available or if you want to switch to a \\ndiffer. \\nTo build a chain, you simply combine the desired components \\nin the order they should be executed. Each component in the \\nchain takes the output of the previous component as input, \\nallowing for a seamless flow of data and interaction with the \\nlanguage model. \\nE. Memory  \\nThe ability to remember prior exchanges conversation is \\nreferred to as memory  [12]. LangChain includes several \\nprograms for increasing system memory. These utilities can \\nbe used independently or as a part of a chain.  We call this \\nability to store information about past interactions \"memory\". \\nLangChain provides a lot of utilities for adding memory to a \\nsystem. These utilities can be used by themselves or \\nincorporated seamlessly into a chain. \\nA memory system must support two fundamental \\nactions: reading and writing. Remember that each chain has \\nsome fundamental execution mechanism that requires \\nspecific inputs. Some of these inputs are provided directly by \\nthe user, while others may be retrieve d from memory. In a \\nsingle run, a chain will interact with its memory system twice. \\n1. A chain will READ from its memory system and \\naugment the user inputs AFTER receiving the initial \\nuser inputs but BEFORE performing the core logic. \\n2. After running the basic logic but before providing the \\nsolution, a chain will WRITE the current run\\'s inputs \\nand outputs to memory so that they may be referred \\nto in subsequent runs. \\nAny memory system\\'s two primary design decisions are: \\n1. How state is stored ? \\nStoring: List of chat messages: A history of all chat \\nexchanges is behind each memory. Even if not all of \\nthese are immediately used, they must be preserved \\nin some manner. A series of integrations for storing \\nthese conversation messages, ranging from in -\\nmemory lists to persistent databases, is a significant \\ncomponent of the LangChain memory module. \\n2. How state is queried ? \\nQuerying: Data structures and algorithms on top of \\nchat messages: Keeping track of chat messages is a \\nsimple task. What is less obvious are the data \\nstructures and algorithms built on top of chat \\nconversations to provide the most usable view of \\nthose chats. \\nA simple memory system may only return the most \\nrecent messages on each iteration. A slightly more \\ncomplicated memory system may return a brief summary of \\nthe last K messages. A more complex system might extract \\nentities from stored messages and only retur n information \\nabout entities that have been referenced in the current run. \\nThere are numerous sorts of memories. Each has its own set \\nof parameters and return types and is helpful in a variety of \\nsituations.  \\nMemory Types:  \\n‚Ä¢ ConversationBufferMemory allows for saving \\nmessages and then extracts the messages in a \\nvariable. \\n‚Ä¢ ConversationBufferWindowMemory keeps a list of \\nthe interactions of the conversation over time. It only \\nuses the last K interactions. This can be useful for \\nkeeping a sliding window of the most recent \\ninteractions, so the buffer does not get too large. \\nThe MindGuide chatbot uses conversation buffer memory. \\nThis memory allows for storing messages and then extracts \\nthe messages in a variable. \\nIII. ARCHITETURE \\nIn crafting the architecture of the MindGuide app, each \\nstep is meticulously designed to create a seamless and \\neffective user experience for those seeking mental health \\nsupport. The user interface, built on Streamlit, sets the tone \\nwith a friendly and safe welcome. Users can jump in by typing \\nWelcome! to your therapy session. I\\'m here to listen, \\nsupport, and guide you through any mental health \\nchallenges or concerns you may have. Please feel free \\nto share what\\'s on your mind, and we\\'ll work together \\nto address your needs. Remember, this is a safe and \\nconfidential space for you to express y ourself. Let\\'s \\nbegin when you\\'re ready.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='their mental health questions, kicking off a series of \\ninteractions with the LangChain framework. This is where the \\nmagic happens ‚Äì LangChain acts as the brain behind the \\nchatbot, working through various components like chat \\nmessage templates and a memory concept to create a \\npersonalized and responsive support system.  Each step is \\nbroken down. \\nStep 1. User Interface:  Developed using the Streamlit \\nframework, the user interface welcomes users with a \\nmessage explaining the role of the chatbot in providing \\nmental health support. It assures users of a safe and \\nconfidential space to express their concerns.  \\nStep 2. User Input - Prompt: Users can input mental health-\\nrelated questions or seek advice by typing their queries \\ninto the input box integrated into the Streamlit interface. \\nStep 3. Data Transfer to LangChain: Implement the \\nfunctionality that sends the user\\'s input (question) as a \\nchat prompt template to the LangChain framework. This \\ninput serves as the \"human message prompt\" template. \\nStep 4. LangChain Framework: In this phase, the LangChain \\nframework serves as the backbone of the chatbot, where \\nall the foundational components and building blocks are \\nmeticulously orchestrated. Here\\'s a deeper dive into the \\ncritical elements of LangChain Processing: \\n‚Ä¢ ChatMessage and Prompt Templates:  Within \\nLangChain, the chatbot\\'s core communication \\ninfrastructure is established by  creating \\nChatMessage and prompt templates for optimal \\nchatbot engagement. \\n‚Ä¢ LLMChain and LLM Model Interaction:  To \\nfacilitate interactions with the large language \\nmodel (LLM), a specialized component called \\nLLMChain is constructed. The LLMChain acts \\nas a conduit for managing the flow of \\nconversation between the chatbot and the LLM \\nmodel, in this case, GPT-4. \\n‚Ä¢ The LLMChain handles both the user\\'s queries \\nand the chatbot\\'s responses, allowing for a \\ndynamic and coherent conversation flow. \\n‚Ä¢ Chatmodel Class of LangChain: The LangChain \\nframework leverages the Chatmodel  class, a \\ncritical component for interfacing with the \\nOpenAI model (GPT-4) for making requests to \\nthe language model and processing its \\nresponses, ensuring seamless communication \\nbetween the chatbot and the AI model. \\n‚Ä¢ Memory Concept:  To enhance the chatbot\\'s \\nconversational capabilities and provide context-\\naware responses, LangChain incorporates a \\nmemory concept that allows the chatbot to retain \\nand access information from past interactions \\nwithin a session. The memory function enhances \\nconversations by retaining user queries, \\npreferences, and contextual details, thereby \\ncontributing to a more effective and \\npersonalized interaction . This way, it tailors \\nresponses based on the user\\'s history throughout \\nthe session. \\nStep 5. Utilize the user\\'s question as input to construct a \\nchain of prompts that the large language model (in this \\ncase, GPT-4) will process. \\nStep 6. Model Response:  Dispatch the constructed input \\nchain to the GPT -4 model for natural language \\nunderstanding and generation.  The GPT -4 model \\ngenerates a response based on the input and context. \\nStep 7. Response to Streamlit:  Receive the response \\ngenerated by the GPT -4 model and transmit it back to \\nthe Streamlit framework for display to the user. \\nStep 8. User Response Delivery:  Present the model -\\ngenerated response to the user, thereby delivering the \\nmental health advice or information they sought.\\n \\nFigure 3. MindGuide Chatbot Architecture'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content=\"IV. STREAMLIT \\nStreamlit [13] is a faster way to build and share data apps. \\nStreamlit turns data scripts into shareable web apps in \\nminutes. Streamlit is an open -source Python library that \\nsimplifies the process of designing and sharing visually \\nappealing web applications, particularly well -suited for \\napplications involving machine learning and data science.  \\nLeveraging Streamlit's Python-based development approach, \\nyou can harness the power of Python to build a responsive and \\ndynamic web application. This is advantageous for developers \\nfamiliar with Python, as it allows for quick and efficient \\ndevelopment. \\n \\nV. MINDGUIDE CHATBOT INTERACTION  \\nThe MindGuide Bot interaction is illustrated in Fig. 4, \\ndepicting the following key elements:  \\n‚Ä¢ Welcome screen interface with AI message and \\nthe initial human interaction with MindGuide \\nChatbot (Fig. 4a). \\n‚Ä¢ MindGuide Chatbot's AI response to the human \\nmessage, followed by the human's mental health \\nquestion (Fig. 4b). \\n‚Ä¢ MindGuide Chatbot's AI response to the \\nsubsequent human message, followed by another \\nmental health question from the human (Fig. 4c). \\n‚Ä¢ MindGuide Chatbot's AI response after \\nanalyzing the latest human message (Fig. 4d). \\n \\n   s \\n                                                         (a)      (b) \\n      \\n                                                         (c)      (d) \\nFigure 4. Sequential Interaction with MindGuide Chatbot - (a) Welcome screen and initial AI message, (b) AI response to the first human message and \\nmental health question, (c) Subsequent AI response and continued interaction with another human mental health question, (d) AI response after analyzing the \\nlatest human message. \\nVI. CONCLUSION \\nThis paper employs the OpenAI chat model GPT-4 with a \\ntemperature setting of 0.5 to serve as an initial therapist, \\nproviding support for patients dealing with mental health \\nissues such as depression and anxiety. MindGuide relies on \\nthe ChatOpenAI model from LangChain as its foundation , \\nincorporating innovative features like ChatPrompt Template, \\nHuman Message Prompt Template, Conversation Buffer \\nMemory, and LLMChain to proactively identify issues and \\ndeliver comprehensive assistance. In the next phase, we plan \\nto enhance this chatbot fu rther by implementing Retrieval -\\nAugmented Generation (RAG) and incorporating embedding \\nvectors for frequently asked questions related to mental health.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='REFERENCES \\n[1] K. Windfuhr and N. Kapur, \"Suicide and mental illness: a clinical \\nreview of 15 years findings from the UK National Confidential Inquiry \\ninto Suicide,\" British medical bulletin, vol. 100, pp. 101-121, 2011. \\n[2] M. D. Choudhury, E. Kiciman, M. Dredze, G. Coppersmith, and M. \\nKumar, \"Discovering shifts to suicidal ideation from mental health \\ncontent in social media,\" in Proceedings of the 2016 CHI Conference \\non Human Factors in Computing Systems, 2016, pp. 2098-2110. \\n[3] S. Ji, C. P. Yu, S. F. Fung, S. Pan, and G. Long, \"Supervised learning \\nfor suicidal ideation detection in online user content,\" Complex, 2018. \\n[4] LangChain, https://www.langchain.com/ (accessed Nov. 29, 2023). \\n[5] LangChain ChatModels, https://blog.langchain.dev/chat-models/ \\n(accessed Nov. 29, 2023). \\n[6] LangChain with OpenAI Chat Model, \\nhttps://python.langchain.com/docs/integrations/chat/openai/ (accessed \\nNov. 29, 2023). \\n[7] LangChain‚Äôs Prompt, https://python.langchain.com/docs/modules \\n/model_io/prompts/ (accessed Nov. 29, 2023). \\n[8] LangChain‚Äôs Chains, https://python.langchain.com/docs/modules \\n/chains (accessed Nov. 29, 2023). \\n[9] OpenAI, https://platform.openai.com/docs/quickstart?context=python \\n(accessed Nov. 29, 2023). \\n[10] LangChain‚Äôs Message Prompt Template,  \\nhttps://python.langchain.com/docs/modules/model_io/prompts/messa\\nge_prompts (accessed Nov. 29, 2023). \\n[11] LangChain‚Äôs Large Language Model Chain,  \\nhttps://python.langchain.com/docs/modules/chains/foundational/llm_c\\nhain (accessed Nov. 29, 2023). \\n[12] Streamlit, https://streamlit.io/ (accessed Nov. 29, 2023).')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PyPDFLoader instance by passing the URL of the PDF file\n",
    "# The loader will download the PDF from the specified URL and prepare it for loading\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "\n",
    "# Call the load() method to:\n",
    "# 1. Download the PDF if needed\n",
    "# 2. Extract text from each page\n",
    "# 3. Create a list of Document objects, one for each page of the PDF\n",
    "# Each Document will contain the text content of a page and metadata including page number\n",
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3879113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2. An AIMessage illustration \n",
      "C. Prompt Template \n",
      "Prompt templates [10] allow you to structure input for LLMs. \n",
      "They provide a convenient way to format user inputs and \n",
      "provide instructions to generate responses. Prompt templates \n",
      "help ensure that the LLM understands the desired context and \n",
      "produces relevant outputs. \n",
      "The prompt template classes in LangChain are built to \n",
      "make constructing prompts with dynamic inputs easier. Of \n",
      "these classes, the simplest is the PromptTemplate. \n",
      "D. Chain \n",
      "Chains [11] in LangChain refer to the combination of \n",
      "multiple components to achieve specific tasks. They provide \n",
      "a structured and modular approach to building language \n",
      "model applications. By combining different components, you \n",
      "can create chains that address various u se cases and \n",
      "requirements. Here are some advantages of using chains: \n",
      "‚Ä¢ Modularity: Chains allow you to break down \n",
      "complex tasks into smaller, manageable \n",
      "components. Each component can be developed and \n",
      "tested independentl\n"
     ]
    }
   ],
   "source": [
    "print(document[2].page_content[:1000])  # print the page 2's first 1000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7f4c8",
   "metadata": {},
   "source": [
    "`URL and website loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e63d9b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.2/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nIntroduction | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentA newer LangChain version is out! Check out the latest version.IntegrationsAPI referenceLatestLegacyMorePeopleContributingCookbooks3rd party tutorialsYouTubearXivv0.2Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsOverview of v0.2Release policyPydantic compatibilityMigrating to v0.2Migrating to LangChain v0.2astream_events v2ChangesMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainSecurityThis is documentation for LangChain v0.2, which is no longer actively maintained.For the current stable version, see this version (Latest).IntroductionOn this pageIntroductionLangChain is a framework for developing applications powered by large language models (LLMs).LangChain simplifies every stage of the LLM application lifecycle:Development: Build your applications using LangChain\\'s open-source building blocks, components, and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.Productionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.Concretely, the framework consists of the following open-source libraries:langchain-core: Base abstractions and LangChain Expression Language.langchain-community: Third party integrations.Partner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.langchain: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.LangGraph: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.LangServe: Deploy LangChain chains as REST APIs.LangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.noteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.Tutorials\\u200bIf you\\'re looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.These are the best ones to get started with:Build a Simple LLM ApplicationBuild a ChatbotBuild an AgentIntroduction to LangGraphExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.How-to guides\\u200bHere you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks.Check out LangGraph-specific how-tos here.Conceptual guide\\u200bIntroductions to all the key parts of LangChain you‚Äôll need to know! Here you\\'ll find high level explanations of all LangChain concepts.For a deeper dive into LangGraph concepts, check out this page.API reference\\u200bHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.Ecosystem\\u200bü¶úüõ†Ô∏è LangSmith\\u200bTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.ü¶úüï∏Ô∏è LangGraph\\u200bBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.Additional resources\\u200bVersions\\u200bSee what changed in v0.2, learn how to migrate legacy code, and read up on our release/versioning policies, and more.Security\\u200bRead up on security best practices to make sure you\\'re developing safely with LangChain.Integrations\\u200bLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.Contributing\\u200bCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.NextTutorialsTutorialsHow-to guidesConceptual guideAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional resourcesVersionsSecurityIntegrationsContributingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a WebBaseLoader instance by passing the URL of the web page to load\n",
    "# This URL points to the LangChain documentation's introduction page\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "\n",
    "# Call the load() method to:\n",
    "# 1. Send an HTTP request to the specified URL\n",
    "# 2. Download the HTML content\n",
    "# 3. Parse the HTML to extract meaningful text\n",
    "# 4. Create a list of Document objects containing the extracted content\n",
    "web_data = loader.load()\n",
    "web_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4cfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 1000 characters of the page content from the first Document\n",
    "# This provides a preview of the successfully loaded web content\n",
    "# web_data[0] accesses the first Document in the list\n",
    "# .page_content accesses the text content of that Document\n",
    "# [:1000] slices the string to get only the first 1000 characters\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56feec18",
   "metadata": {},
   "source": [
    "`Text splitters`: chunks should be created with some overlap to keep context between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca8d0e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "# Create a CharacterTextSplitter with specific configuration:\n",
    "# - chunk_size=200: Each chunk will contain approximately 200 characters\n",
    "# - chunk_overlap=20: Consecutive chunks will overlap by 20 characters to maintain context\n",
    "# - separator=\"\\n\": Text will be split at newline characters when possible\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")\n",
    "\n",
    "# Split the previously loaded document (PDF or other text) into chunks\n",
    "# The split_documents method:\n",
    "# 1. Takes a list of Document objects\n",
    "# 2. Splits each document's content based on the configured parameters\n",
    "# 3. Returns a new list of Document objects where each contains a chunk of text\n",
    "# 4. Preserves the original metadata for each chunk\n",
    "chunks = text_splitter.split_documents(document)\n",
    "\n",
    "# Print the total number of chunks created\n",
    "# This shows how many smaller Document objects were generated from the original document(s)\n",
    "# The number depends on the original document length and the chunk_size setting\n",
    "print(len(chunks))\n",
    "# chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f97ca6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks size approximation: 139.9611111111111\n"
     ]
    }
   ],
   "source": [
    "total_char = 0\n",
    "for page in document:\n",
    "    for token in page.page_content:\n",
    "        # print(token, end='-')\n",
    "        total_char += len(token)\n",
    "    # print('\\n\\n\\n<<===============================>>\\n\\n\\n')\n",
    "print(f\"Chunks size approximation: {total_char / (200 - 20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29264412",
   "metadata": {},
   "source": [
    "#### A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c85c4f",
   "metadata": {},
   "source": [
    "#### A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade7d32",
   "metadata": {},
   "source": [
    "#### A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
