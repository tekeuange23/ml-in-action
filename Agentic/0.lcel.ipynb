{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ff7bbf",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a6a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf45f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''     Env setup and Gemini model initialization     '''\n",
    "load_dotenv() \n",
    "\n",
    "# Check if the API key is loaded\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    print(\"Error: GOOGLE_API_KEY not found in environment variables.\")\n",
    "    exit()\n",
    "\n",
    "# models: ['gemini-2.5-pro', 'gemma-3-27b-it']\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemma-3-27b-it\", \n",
    "    temperature=0.7, \n",
    ")\n",
    "\n",
    "# StrOutputParser() simply extracts the string content from the AIMessage\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a0a98c",
   "metadata": {},
   "source": [
    "### 1. simple invocation (basic prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4071ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most basic way to call the model\n",
    "result = llm.invoke(\"Who was the first person to walk on the moon?\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab2d82",
   "metadata": {},
   "source": [
    "### 2. build chain with the prompt template (Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66035bea",
   "metadata": {},
   "source": [
    "#### ---> basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most basic way to call the model\n",
    "\n",
    "# Create a template for our prompt. The {topic} part is a variable.\n",
    "template = \"Tell me a short joke about {topic}.\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "\n",
    "# Create the chain by piping the prompt to the language model\n",
    "# The output of the prompt will be the input for the llm\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain by passing the value for our 'topic' variable\n",
    "response = chain.invoke({\"topic\": \"a programmer\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4b96f",
   "metadata": {},
   "source": [
    "#### ---> adding output parser for cleaner output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the chain, adding the parser at the end\n",
    "chain_with_parser = prompt | llm | parser\n",
    "\n",
    "# Invoke the new chain\n",
    "clean_response = chain_with_parser.invoke({\"topic\": \"a cat\"})\n",
    "\n",
    "print(f\"Type of response: {type(clean_response)}\")\n",
    "print(\"Response:\")\n",
    "print(clean_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63a1cc",
   "metadata": {},
   "source": [
    "#### ---> advance chain (passing through input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if you want to see both the original question and the answer?\n",
    "# We can use RunnablePassthrough to pass the input topic along the chain.\n",
    "print(\"\\n=========        Chain with Passthrough       =========\")\n",
    "\n",
    "final_chain = (\n",
    "    {\"joke\": prompt | llm | parser} \n",
    "    | RunnablePassthrough()\n",
    ")\n",
    "\n",
    "# The result is now a dictionary containing the generated joke\n",
    "result_dict = final_chain.invoke({\"topic\": \"a robot\"})\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4dab0",
   "metadata": {},
   "source": [
    "### 3. sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c07c8",
   "metadata": {},
   "source": [
    "### 4. parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7bec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINE THE INPUT TEXT ---\n",
    "input_text = \"\"\"\n",
    "The new SuperGraphX 5000 is a revolutionary graphics card. \n",
    "It delivers breathtaking visuals and unparalleled performance, making every game an immersive experience.\n",
    "However, its high price point of $1200 might be a deterrent for budget-conscious builders. \n",
    "The power consumption is also noticeably high, requiring a robust power supply.\n",
    "\"\"\"\n",
    "\n",
    "# --- BUILD THE PARALLEL CHAIN (Your exact idea, formalized) ---\n",
    "\n",
    "# This is the canonical way to write what you proposed.\n",
    "# We define a dictionary where each value is a separate chain.\n",
    "# We add StrOutputParser to get clean string outputs.\n",
    "parallel_chain = RunnableParallel({\n",
    "    'summary': ChatPromptTemplate.from_template('Summarise this text in one sentence: {text}') | llm | parser,\n",
    "    'translation': ChatPromptTemplate.from_template('Translate this text into French: {text}') | llm | parser,\n",
    "    'sentiment': ChatPromptTemplate.from_template('What is the overall sentiment in this text? (positive, negative or neutral): {text}') | llm | parser,\n",
    "    'keywords': ChatPromptTemplate.from_template('Extract 5 main keywords from this text, separated by commas: {text}') | llm | parser,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d628d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parallel execution took: 16.40 seconds\n",
      "\n",
      "--- Results ---\n",
      "\n",
      "[Summary]:\n",
      "The SuperGraphX 5000 is a high-performing graphics card offering stunning visuals, but its $1200 price and high power consumption may limit its appeal to serious gamers with substantial budgets and capable systems.\n",
      "\n",
      "[French Translation]:\n",
      "Here are a few options for the translation, ranging from more literal to slightly more fluid. I've included notes on the nuances:\n",
      "\n",
      "**Option 1 (More Literal):**\n",
      "\n",
      "> La nouvelle SuperGraphX 5000 est une carte graphique révolutionnaire.\n",
      "> Elle offre des visuels époustouflants et des performances inégalées, transformant chaque jeu en une expérience immersive.\n",
      "> Cependant, son prix élevé de 1200 $ pourrait être un frein pour les constructeurs soucieux de leur budget.\n",
      "> La consommation d'énergie est également notablement élevée, nécessitant une alimentation robuste.\n",
      "\n",
      "* **Strengths:** Very accurate to the original meaning.\n",
      "* **Weaknesses:**  A little bit stiff in places. \"Un frein\" is perfectly correct, but can sound a bit formal.\n",
      "\n",
      "**Option 2 (More Fluid & Common):**\n",
      "\n",
      "> La nouvelle SuperGraphX 5000 est une carte graphique révolutionnaire.\n",
      "> Elle propose des graphismes à couper le souffle et des performances exceptionnelles, rendant chaque jeu incroyablement immersif.\n",
      "> Toutefois, son prix de 1200 $ risque de décourager les utilisateurs ayant un budget limité.\n",
      "> Sa consommation électrique est également assez élevée et exige une alimentation puissante.\n",
      "\n",
      "* **Strengths:**  Sounds more natural in French.  \"À couper le souffle\" is a common idiom for \"breathtaking.\" \"Décourager\" is a more common way to say \"deter.\" \"Puissante\" is a good alternative to \"robuste\" for power supply.\n",
      "* **Weaknesses:** Slightly less literal than Option 1, but the meaning is preserved.\n",
      "\n",
      "**Option 3 (Slightly more marketing-focused):**\n",
      "\n",
      "> La SuperGraphX 5000, nouvelle carte graphique, est une véritable révolution.\n",
      "> Elle offre des images spectaculaires et des performances inégalées pour une immersion totale dans vos jeux.\n",
      "> Néanmoins, son prix de 1200 $ pourrait constituer un obstacle pour les joueurs soucieux de leur budget.\n",
      "> Sa forte consommation d'énergie requiert également une alimentation de qualité.\n",
      "\n",
      "* **Strengths:**  Sounds very polished and suitable for marketing materials. Uses stronger adjectives (\"spectaculaires,\" \"totale\").\n",
      "* **Weaknesses:**  The most interpretive of the three.\n",
      "\n",
      "\n",
      "\n",
      "**Key Differences & Explanations:**\n",
      "\n",
      "* **\"Breathtaking visuals\":**  \"Visuels époustouflants\" (Option 1) is accurate. \"Graphismes à couper le souffle\" (Option 2) and \"images spectaculaires\" (Option 3) are more idiomatic and sound better.\n",
      "* **\"Unparalleled performance\":** \"Performances inégalées\" is a good translation. \"Exceptionnelles\" (Option 2) is also very common.\n",
      "* **\"Deterrent\":** \"Frein\" (Option 1) is correct but a bit formal. \"Décourager\" (Option 2) and \"obstacle\" (Option 3) are more natural.\n",
      "* **\"Budget-conscious builders\":** \"Constructeurs soucieux de leur budget\" (Option 1) is accurate. \"Utilisateurs ayant un budget limité\" (Option 2) and \"joueurs soucieux de leur budget\" (Option 3) are more common phrasing.\n",
      "* **\"Robust power supply\":** \"Alimentation robuste\" (Option 1) is fine. \"Alimentation puissante\" (Option 2) and \"alimentation de qualité\" (Option 3) are more natural.\n",
      "\n",
      "\n",
      "\n",
      "**Recommendation:**\n",
      "\n",
      "I recommend **Option 2** as the best overall translation. It strikes a good balance between accuracy and natural-sounding French.  However, if you're writing marketing copy, **Option 3** would be a good choice.\n",
      "\n",
      "[Sentiment]:\n",
      "The overall sentiment is **slightly positive**. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Strong Positives:** The text leads with very enthusiastic praise (\"revolutionary,\" \"breathtaking,\" \"unparalleled,\" \"immersive\"). These are powerful positive descriptors.\n",
      "* **Acknowledged Negatives:** The negatives (price and power consumption) are presented as *drawbacks* to an otherwise excellent product. They are framed as considerations, not deal-breakers.\n",
      "\n",
      "While the price and power consumption are downsides, the initial and more impactful impression is overwhelmingly positive due to the strong language used to describe the card's performance.\n",
      "\n",
      "[Keywords]:\n",
      "Graphics card, SuperGraphX 5000, performance, price, power consumption.\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTE THE CHAIN AND MEASURE TIME ---\n",
    "start_time = time.time()\n",
    "# The input dictionary key 'text' is passed to every prompt template in the parallel_chain\n",
    "results = parallel_chain.invoke({'text': input_text})\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nParallel execution took: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# --- DISPLAY THE RESULTS ---\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"\\n[Summary]:\\n{results['summary']}\")\n",
    "print(f\"\\n[French Translation]:\\n{results['translation']}\")\n",
    "print(f\"\\n[Sentiment]:\\n{results['sentiment']}\")\n",
    "print(f\"\\n[Keywords]:\\n{results['keywords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a97c3",
   "metadata": {},
   "source": [
    "### 5. Role playing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(variables):\n",
    "    return prompt.format(**variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d66cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"\"\"\n",
    "    Dungeon & Dragons game master\n",
    "\"\"\"\n",
    "\n",
    "tone = \"engaging and immersive\"\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an expert {role}. I have this question {question}. I would like our conversation to be {tone}.\n",
    "    \n",
    "    Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "roleplay_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive chat loop\n",
    "while True:\n",
    "    query = input(\"Question: \")\n",
    "    \n",
    "    if query.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "        print(\"Answer: Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    response = roleplay_chain.invoke({\"role\": role, \"question\": query, \"tone\": tone})\n",
    "    print(\"Answer: \", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
