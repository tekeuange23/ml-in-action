{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed5e2a1",
   "metadata": {},
   "source": [
    "### Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install ibm-watsonx-ai==0.2.6\n",
    "# !pip install langchain==0.1.16\n",
    "# !pip install langchain-ibm==0.1.4\n",
    "# # !pip install transformers==4.41.2\n",
    "# !pip install huggingface-hub==0.23.4\n",
    "# # !pip install sentence-transformers==2.5.1\n",
    "# !pip install chromadb\n",
    "# !pip install wget==3.2\n",
    "# # !pip install --upgrade torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b285506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                                0.3.25\n",
      "langchain-community                      0.3.25\n",
      "langchain-core                           0.3.78\n",
      "langchain-experimental                   0.3.4\n",
      "langchain-google-genai                   2.0.10\n",
      "langchain-ollama                         0.3.10\n",
      "langchain-openai                         0.3.34\n",
      "langchain-text-splitters                 0.3.11\n",
      "langchainhub                             0.1.21\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c1910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "# from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e29c4",
   "metadata": {},
   "source": [
    "### Load the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e66c0",
   "metadata": {},
   "source": [
    "The document, which is provided in a TXT format, outlines some company policies and serves as an example data set for the project.\n",
    "\n",
    "This is the `load` step in `Indexing`.<br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MPdUH7bXpHR5muZztZfOQg.png\" width=\"50%\" alt=\"split\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c5cac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file downloaded\n"
     ]
    }
   ],
   "source": [
    "filename = 'companyPolicies.txt'\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'\n",
    "\n",
    "# Use wget to download the file\n",
    "wget.download(url, out=filename)\n",
    "print('file downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3925b3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    contents = file.read()\n",
    "    print(type(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d95a6e",
   "metadata": {},
   "source": [
    "### Splitting the document into chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc31fee",
   "metadata": {},
   "source": [
    "In this step, you are splitting the document into chunks, which is basically the `split` process in `Indexing`.\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/0JFmAV5e_mejAXvCilgHWg.png\" width=\"50%\" alt=\"split\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1e92a",
   "metadata": {},
   "source": [
    "`LangChain` is used to split the document and create chunks. It helps you divide a long story (document) into smaller parts, which are called `chunks`, so that it's easier to handle. \n",
    "\n",
    "For the splitting process, the goal is to ensure that each segment is as extensive as if you were to count to a certain number of characters and meet the split separator. This certain number is called `chunk size`. Let's set 1000 as the chunk size in this project. Though the chunk size is 1000, the splitting is happening randomly. This is an issue with LangChain. `CharacterTextSplitter` uses `\\n\\n` as the default split separator. You can change it by adding the `separator` parameter in the `CharacterTextSplitter` function; for example, `separator=\"\\n\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35dd9811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1624, which is longer than the specified 1000\n",
      "Created a chunk of size 1885, which is longer than the specified 1000\n",
      "Created a chunk of size 1903, which is longer than the specified 1000\n",
      "Created a chunk of size 1729, which is longer than the specified 1000\n",
      "Created a chunk of size 1678, which is longer than the specified 1000\n",
      "Created a chunk of size 2032, which is longer than the specified 1000\n",
      "Created a chunk of size 1894, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edeb643",
   "metadata": {},
   "source": [
    "### Embedding and storing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced3b77",
   "metadata": {},
   "source": [
    "This step is the `embed` and `store` processes in `Indexing`. <br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/u_oJz3v2cSR_lr0YvU6PaA.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22bf27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document ingested\n"
     ]
    }
   ],
   "source": [
    "# Use your Ollama server URL/model\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:v1.5\", base_url=\"http://localhost:11434\") \n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings)  # store the embedding in docsearch using Chromadb\n",
    "print('document ingested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d827503",
   "metadata": {},
   "source": [
    "### LLM model construction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c6fd5",
   "metadata": {},
   "source": [
    "This completes the `LLM` part of the `Retrieval` task. <br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/UZXQ44Tgv4EQ2-mTcu5e-A.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_llm = ChatOllama(model=\"llama3.2\")\n",
    "deepseek_r1_llm = ChatOllama(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf267583",
   "metadata": {},
   "source": [
    "### Integrating LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3548ce",
   "metadata": {},
   "source": [
    "LangChain has a number of components that are designed to help retrieve information from the document and build question-answering applications, which helps you complete the `retrieve` part of the `Retrieval` task. <br>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/M4WpkkMMbfK0Wkz0W60Jiw.png\" width=\"50%\" alt=\"split\"/>\n",
    "\n",
    "In the following steps, you create a simple Q&A application over the document source using LangChain's `RetrievalQA`.\n",
    "\n",
    "Then, you ask the query \"what is mobile policy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cea85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llama_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(), \n",
    "                                 return_source_documents=False)\n",
    "query = \"what is mobile policy?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8761728",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=deepseek_r1_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(), \n",
    "                                 return_source_documents=False)\n",
    "query = \"Can you summarize the document for me?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20d9bd",
   "metadata": {},
   "source": [
    "### Dive deeper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb61b3f",
   "metadata": {},
   "source": [
    "How to add the prompt in retrieval using LangChain? <br>\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bvw3pPRCYRUsv-Z2m33hmQ.png\" width=\"50%\" alt=\"split\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ef036",
   "metadata": {},
   "source": [
    "Use prompts to guide the responses from an LLM the way you want. For instance, if the LLM is uncertain about an answer, you instruct it to simply state, \"I do not know,\" instead of attempting to generate a speculative response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query is asking something that does not exist in the document. \n",
    "# The LLM responds with information that actually is not true. \n",
    "# we don't want this to happen, so you must add a prompt to the LLM.\n",
    "qa = RetrievalQA.from_chain_type(llm=llama_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(), \n",
    "                                 return_source_documents=False)\n",
    "query = \"Can I eat in company vehicles?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4835d",
   "metadata": {},
   "source": [
    "#### `Prompt Template`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the information from the document to answer the question at the end. If you don't know the answer, just say that you don't know, definately do not try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llama_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(), \n",
    "                                 chain_type_kwargs=chain_type_kwargs, \n",
    "                                 return_source_documents=False)\n",
    "\n",
    "query = \"Can I eat in company vehicles?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e2178f",
   "metadata": {},
   "source": [
    "#### `Make the conversation have memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85d509",
   "metadata": {},
   "source": [
    "For conversations with an LLM to be more like a dialogue with a friend who remembers what you talked about last time? An LLM that retains the memory of previous exchanges builds a more coherent and contextually rich conversation.\n",
    "\n",
    "To make the LLM have memory, you introduce the `ConversationBufferMemory` function from LangChain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62502eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb267f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llama_llm, \n",
    "                                           chain_type=\"stuff\", \n",
    "                                           retriever=docsearch.as_retriever(), \n",
    "                                           memory = memory, \n",
    "                                           get_chat_history=lambda h : h, \n",
    "                                           return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is mobile policy?\"\n",
    "result = qa.invoke({\"question\":query}, {\"chat_history\": history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d536d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append((query, result[\"answer\"])) # Append the previous query and answer to the chat history again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b779bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"List points in it?\"\n",
    "result = qa({\"question\": query}, {\"chat_history\": history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb85244",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append((query, result[\"answer\"])) # Append the previous query and answer to the chat history again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dea637",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the aim of it?\"\n",
    "result = qa({\"question\": query}, {\"chat_history\": history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf809a",
   "metadata": {},
   "source": [
    "`Return the source from the document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3621673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['source_documents'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba731a1",
   "metadata": {},
   "source": [
    "### Chat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e412bd",
   "metadata": {},
   "source": [
    "An agent which can retrieve information from the document and has the conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa():\n",
    "    memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm=llama_llm, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=docsearch.as_retriever(), \n",
    "                                               memory = memory, \n",
    "                                               get_chat_history=lambda h : h, \n",
    "                                               return_source_documents=False)\n",
    "    history = []\n",
    "    while True:\n",
    "        query = input(\"Question: \")\n",
    "        \n",
    "        if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n",
    "            print(\"Answer: Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        result = qa({\"question\": query}, {\"chat_history\": history})\n",
    "        \n",
    "        history.append((query, result[\"answer\"]))\n",
    "        \n",
    "        print(\"Answer: \", result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
