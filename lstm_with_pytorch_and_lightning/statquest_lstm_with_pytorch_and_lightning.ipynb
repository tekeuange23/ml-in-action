{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34006ee1-51e5-4a48-82de-021a71e1201a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# StatQuest: Long Short-Term Memory (LSTM) with PyTorch + Lightning!!!\n",
    "## Sponsored by...\n",
    "[<img src=\"./images/Brandmark_FullColor_Black.png\" alt=\"Lightning\" style=\"width: 400px;\">](https://lightning.ai/)\n",
    "\n",
    "Copyright 2022, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
   "metadata": {},
   "source": [
    "---- \n",
    "**NOTE:** This tutorial is from the StatQuest **[Long Short-Term Memory with PyTorch + Lightning](https://youtu.be/RHGiXPuo_pI)**.\n",
    "\n",
    "In this tutorial, we will use **[PyTorch](https://pytorch.org/) + [Lightning](https://lightning.ai/)** to create, optimize and make predictions using the **Long Short-Term Memory (LSTM)** network featured in the **StatQuest** **[Long Short-Term Memory, Clearly Explained!!!](https://youtu.be/YCzL96nL7j0)** In that **StatQuest**, we implemented a **Long Short-Term Memory** unit, seen below, that uses predicts sequential data to predict the value of two different companies.\n",
    "<!-- <img src=\"./xgboost_tree.png\" alt=\"An XGBoost Tree\" style=\"width: 600px;\"> -->\n",
    "\n",
    "<img src=\"./images/lstm_image.001.png\" alt=\"A Long Short-Term Memory Unit\" style=\"width: 1620px;\">\n",
    "\n",
    "The training data (below) consist of stock prices for two different companies, **Company A** and **Company B**. The goal is to use the data from the first **4** days to predict what the price will be on the **5**th day. If you look closely at the data, you'll see that the only differences in the prices occur on Day **1** and Day **5**. So the LSTM has to remember what happened on Day **1** in order to predict what will happen on Day **5**.\n",
    "\n",
    "\n",
    "<img src=\"./images/company_a_data.png\" alt=\"Data for Company A\" style=\"width: 450px;\"> <img src=\"./images/company_b_data.png\" alt=\"Data for Company B\" style=\"width: 450px;\">\n",
    "\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Build a Long Short-Term Memory (LSTM) unit by hand with Lightning](#build)**\n",
    "\n",
    "- **[Train the LSTM unit and use Lightning and TensorBoard to evaluate](#train)**\n",
    "\n",
    "- **[Add additional epochs to the training without starting over](#add_epochs)**\n",
    "\n",
    "- **[Build a Long Short-Term Memory Unit with nn.LSTM() and train it with Lightning](#nnLSTM)**\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you already know the basics of coding in **Python** and are familiar with the <!-- basics of **[PyTorch](https://youtu.be/FHdlXe1bSe4)** and the  --> theory behind **[Neural Networks](https://youtu.be/CqOfi41LfDw)**, **[Backpropagation](https://youtu.be/IN2XmBhILt4)**, and **[Long Short-Term Memory](https://youtu.be/YCzL96nL7j0)**. If not, check out the **'Quests** by clicking on the links for each topic.\n",
    "\n",
    "#### ALSO NOTE:\n",
    "I strongly encourage you to play around with the code. Playing with the code is the best way to learn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3669a0-284c-48f7-9c73-19812e2a8fac",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "**TL;DR** For the most part, this is the same as the **The StatQuest Introduction to Coding Neural Networks with PyTorch + Lightning**, except now we don't have to draw our own graphs, so we are omitting `matplotlib` and `seaborn`. Also, instead of using **Stochastic Gradient Descent** to optimize the weights and biaes, we are using **Adam** and we're also importing a function that allows us to set the seed for the random numbers we use to initialize the Weights and Biases. So, with that said...\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create a Long Short-Term Memory (LSTM) neural network and optimize the neural network's parameters.\n",
    "\n",
    "**NOTE:** You will need **Python 3.8** and have at least these versions for each of the following modules: \n",
    "- pytorch >= 1.10.1\n",
    "- lightning >= 1.8.0\n",
    "\n",
    "### If you installed **Python** with [Anaconda](https://www.anaconda.com/)...\n",
    "...then you can check which versions of each package you have with the command: `conda list`. If, for example, your version of `matplotlib` is older than **3.3.4**, then the easiest thing to do is just update all of your Anaconda packages with the following command: `conda update --all`. However, if you only want to update `matplotlib`, then you can run this command: `conda install matplotlib=3.3.4`.\n",
    "\n",
    "### If you need to install **PyTorch**...\n",
    "...then the easiest thing to do is follow the instructions on the [PyTorch website](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "### If you need to install **Lightning**...\n",
    "...then the easiest thing to do is follow the instructions on the [Lightning AI website](https://lightning.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "from pytorch_lightning.utilities.seed import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1076357-c957-44ed-947b-8267b4f4c3a9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4aed1-1e0c-4895-b96f-9dcfca679dd0",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "# Build a Long Short-Term Memory unit by hand using PyTorch + Lightning\n",
    "\n",
    "Just like we have done in previous tutorials, building a neural network, and a Long Short-Term Memory (LSTM) unit is a type of neural network, means we need to create a new class. To make it easy to train the LSTM, this class will inherit from `LightningModule` and we'll create the following methods:\n",
    "- `__init__()` to initialize the Weights and Biases and keep track of a few other house keeping things.\n",
    "- `lstm_unit()` to do the LSTM math. For example, to calculate the percentage of the long-term memory to remember.\n",
    "- `forward()` to make a forward pass through the unrolled LSTM. In other words `forward()` calls `lstm_unit()` for each data point.\n",
    "- `configure_optimizers()` to configure the opimimizer. In the past, we have use `SGD` (Stochastic Gradient Descent), however, in this tutorial we'll change things up and use `Adam`, another popular algorithm for optimizing the Weights and Biases.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss and to keep track of the loss values in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4274850-fac0-4099-87ce-342d0aab2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are implementing an LSTM network by hand...\n",
    "class LSTMbyHand(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        ## The first thing we do is set the seed for the random number generorator.\n",
    "        ## This ensures that when someone creates a model from this class, that model\n",
    "        ## will start off with the exact same random numbers as I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Initialize the tensors for the LSTM\n",
    "        ##\n",
    "        ###################\n",
    "        \n",
    "        ## NOTE: nn.LSTM() uses random values from a uniform distribution to initialize the tensors\n",
    "        ## Here we can do it 2 different ways 1) Normal Distribution and 2) Uniform Distribution\n",
    "        ## We'll start with the Normal Distribtion...\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)        \n",
    "        \n",
    "        ## NOTE: In this case, I'm only using the normal distribution for the Weights.\n",
    "        ## All Biases are initialized to 0.\n",
    "        ##\n",
    "        ## These are the Weights and Biases in the first stage, which determines what percentage\n",
    "        ## of the long-term memory the LSTM unit will remember.\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        ## These are the Weights and Biases in the second stage, which determins the new\n",
    "        ## potential long-term memory and what percentage will be remembered.\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        ## These are the Weights and Biases in the third stage, which determines the\n",
    "        ## new short-term memory and what percentage will be sent to the output.\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        ## We can also initialize all Weights and Biases using a uniform distribution. This is\n",
    "        ## how nn.LSTM() does it.\n",
    "#         self.wlr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wlr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.blr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wpr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wp2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "#         self.wo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wo2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        ## lstm_unit does the math for a single LSTM unit.\n",
    "        \n",
    "        ## NOTES:\n",
    "        ## long term memory is also called \"cell state\"\n",
    "        ## short term memory is also called \"hidden state\"\n",
    "        \n",
    "        ## 1) The first stage determines what percent of the current long-term memory\n",
    "        ##    should be remembered\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) + \n",
    "                                              (input_value * self.wlr2) + \n",
    "                                              self.blr1)\n",
    "        \n",
    "        ## 2) The second stage creates a new, potential long-term memory and determines what\n",
    "        ##    percentage of that to add to the current long-term memory\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) + \n",
    "                                                   (input_value * self.wpr2) + \n",
    "                                                   self.bpr1)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + \n",
    "                                      (input_value * self.wp2) + \n",
    "                                      self.bp1)\n",
    "        \n",
    "        ## Once we have gone through the first two stages, we can update the long-term memory\n",
    "        updated_long_memory = ((long_memory * long_remember_percent) + \n",
    "                       (potential_remember_percent * potential_memory))\n",
    "        \n",
    "        ## 3) The third stage creates a new, potential short-term memory and determines what\n",
    "        ##    percentage of that should be remembered and used as output.\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) + \n",
    "                                       (input_value * self.wo2) + \n",
    "                                       self.bo1)         \n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "        \n",
    "        ## Finally, we return the updated long and short-term memories\n",
    "        return([updated_long_memory, updated_short_memory])\n",
    "        \n",
    "    \n",
    "    def forward(self, input): \n",
    "        ## forward() unrolls the LSTM for the training data by calling lstm_unit() for each day of training data \n",
    "        ## that we have. forward() also keeps track of the long and short-term memories after each day and returns\n",
    "        ## the final short-term memory, which is the 'output' of the LSTM.\n",
    "        \n",
    "        long_memory = 0 # long term memory is also called \"cell state\" and indexed with c0, c1, ..., cN\n",
    "        short_memory = 0 # short term memory is also called \"hidden state\" and indexed with h0, h1, ..., cN\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "        \n",
    "        ## Day 1\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 2\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 3\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 4\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "        \n",
    "        ##### Now return short_memory, which is the 'output' of the LSTM.\n",
    "        return short_memory\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        # return Adam(self.parameters(), lr=0.1) # NOTE: Setting the learning rate to 0.1 trains way faster than\n",
    "                                                 # using the default learning rate, lr=0.001, which requires a lot more \n",
    "                                                 # training. However, if we use the default value, we get \n",
    "                                                 # the exact same Weights and Biases that I used in\n",
    "                                                 # the LSTM Clearly Explained StatQuest video. So we'll use the\n",
    "                                                 # default value.\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        ## NOTE: Our dataset consists of two sequences of values representing Company A and Company B\n",
    "        ## For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,\n",
    "        ## the goal is to predict that the value on Day 5 = 1. We use label_i, the value we want to\n",
    "        ## predict, to keep track of which company we just made a prediction for and \n",
    "        ## log that output value in a company specific file\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74907-ac4c-4dfd-aa53-78eaa1d6cbbb",
   "metadata": {},
   "source": [
    "Once we have created the class that defines an LSTM, we can use it to create a model and print out the randomly initialized Weights and Biases. Then, just for fun, we'll see what those random Weights and Biases predict for **Company A** and **Company B**. If they are good predictions, then we're done! However, the chances of getting good predictions from random values is very small. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ea653-f746-46db-9029-44242a1b8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model object, print out parameters and see how well\n",
    "## the untrained LSTM can make predictions...\n",
    "model = LSTMbyHand() \n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "## NOTE: To make predictions, we pass in the first 4 days worth of stock values \n",
    "## in an array for each company. In this case, the only difference between the\n",
    "## input values for Company A and B occurs on the first day. Company A has 0 and\n",
    "## Company B has 1.\n",
    "print(\"Company A: Observed = 0, Predicted =\", \n",
    "      model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", \n",
    "      model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329d6f2-39ff-4f9b-bc99-fbd1a2013815",
   "metadata": {},
   "source": [
    "With the unoptimized paramters, the predicted value for **Company A**, **-0.0377**, isn't terrible, since it is relatively close to the observed value, **0**. However, the predicted value for **Company B**, **-0.0383**, _is_ terrible, because it is relatively far from the observed value, **1**. So, that means we need to train the LSTM.\n",
    "\n",
    "Small bam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1116de-c45d-4d82-a727-73a5806fa18f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0529b9c-c550-4fc6-a424-069e34b29c4b",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# Train the LSTM unit and use Lightning and TensorBoard to evaluate: Part 1 - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e2424-649a-44ce-8a34-c98b8fced609",
   "metadata": {},
   "source": [
    "Since we are using **Lightning** training, training the LSTM we created by hand is pretty easy. All we have to do is create the training data and put it into a `DataLoader`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea2ef9-7fe2-4bfe-8645-4db921a4b20c",
   "metadata": {},
   "source": [
    "...and then create a **Lightning Trainer**, `L.Trainer`, and fit it to the training data. **NOTE:** We are starting with **2000** epochs. This may be enough to successfully optimize all of the parameters, but it might not. We'll find out after we compare the predictions to the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10157397-3100-4d18-be96-cdc294ce89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=2000) # with default learning rate, 0.001 (this tiny learning rate makes learning slow)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586093b-38e5-431a-a7f0-ad65f2a4c979",
   "metadata": {},
   "source": [
    "Now that we've trained the model with **2000** epochs, we can see how good the predictions are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292d775-336f-41b7-838c-b297c49693a8",
   "metadata": {},
   "source": [
    "Unfortunately, these predictions are terrible. :( So it seems like we'll have to do more training. However, it would be awesome if we could be confident that more training will actually improve the predictions. If not, we can spare ourselves a lot of time, and potentially money, and just give up. So, before we dive into more training, let's look at the loss values and predictions that we saved in log files with **TensorBoard**. **TensorBoard** will graph everything that we logged during training, making it super easy to see if things are headed in the right direction or not.\n",
    "\n",
    "To get TensorBoard working\n",
    "- In the Jupyter browser window, go to the **File** menu and select **New**.\n",
    "- In the submenu, select **Terminal**.\n",
    "- In the terminal, navigate to the same directory that contains the **lightning_logs** directory.\n",
    "- Then in the terminal, enter `tensorboard --logdir=lightning_logs/` to start the **TensorBoard** server.\n",
    "- When the **TensorBoard** server starts, it will print out a URL that looks like this `http://localhost:6006/`. Copy the URL and paste it into a new browser window and then you are good to go!!! \n",
    "- BAM!!!\n",
    "\n",
    "Below are the graphs of **loss** (`train_loss`), the predictions for **Company A** (`out_0`), and the predictions for **Company B** (`out_1`). Remember for **Companay A**, we want to predict **0** and for **Company B**, we want to predict **1**.\n",
    "\n",
    "<img src=\"./images/train_loss_2000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_2000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_2000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "**NOTE:** If your graphs look messed up and you see a bunch of different lines, instead of just one red line per graph, then check where this notebook is saved for a directory called `lightning_logs`. Delete `lightning_logs` and the re-run everything in this notebook. One source of problems with the graphs is that every time we train a model, a new batch of log files is created and stored in `lightning_logs` and **TensorBoard**, by default, will plot all of them. You can turn off unwanted log files in **TensorBoard**, and we'll do this later on in this notebook, but for now, the easiest thing to do is to start with a clean slate.\n",
    "\n",
    "Anyway, if we look at the **loss** (`train_loss`), we see that it is going down, which is good, but it still has further to go. When we look at the predictions for **Company A** (`out_0`), we see that they started out pretty good, close to **0**, but then got really bad early on in training, shooting all the way up to **0.5**, but are starting to get smaller. In contrast, when we look at the predictions for **Company B** (`out_1`), we see that they started out really bad, close to **0**, but have been getting better ever since and look like they could continue to get better if we kept training.\n",
    "\n",
    "In summary, the graphs seem to suggest that if we continued training our model, the predictions would improve. So let's add more epochs to the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ed041-4f46-4dc6-bd6f-7d2bf01886a4",
   "metadata": {},
   "source": [
    "<a id=\"add_epochs\"></a>\n",
    "# Optimizing (Training) the Weights and Biases in the LSTM that we made by hand: Part 2 - Adding More Epochs without Starting Over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4989ee2-f2ce-40c1-b8a6-0f3c4d4e06c6",
   "metadata": {},
   "source": [
    "The good news is that because we're using **Lightning**, we can pick up where we left off training without having to start over from scratch. This is because when we train with **Lightning**, it creates _checkpoint_ files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the `Trainer` where the checkpoint files are located. This is awesome and will save us a lot of time since we don't have to retrain the first **2000** epochs. So let's add an additional **1000** epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff55c1-cdc2-4086-b0d4-8d8333058674",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, find where the most recent checkpoint files are stored\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=3000) # Before, max_epochs=2000, so, by setting it to 3000, we're adding 1000 more.\n",
    "## And then call fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f6af6-7d7e-4c40-9bbf-f467114d838e",
   "metadata": {},
   "source": [
    "Now that we have added **1000** epochs to the training, let's check the predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c805e2b-27ae-4302-af26-5fe0d55f6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42455342-f774-43a7-a043-536099d39b01",
   "metadata": {},
   "source": [
    "...and they are much better than before. Hooray!!! We can also check the logs with **TensorBoard** to see if it makes sense to add more epochs to the training. Since we already have **TensorBoard** running in a separate browser window, all we have to do is reload that page to update the graphs (below).\n",
    "\n",
    "<img src=\"./images/train_loss_3000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_3000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_3000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "The blue lines in each graph represents the values we logged during the extra **1000** epochs. The **loss** is getting smaller and the predictions for both companies are improving! Hooray!!! However, because it looks like there is even more room for improvement, let's add **2000** more epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb14b21-2aa4-4d37-a58c-d2fb789c06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, find where the most recent checkpoint files are stored\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=5000) # Before, max_epochs=3000, so, by setting it to 5000, we're adding 2000 more.\n",
    "## And then call fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231699e4-56eb-4c1b-b48c-e146b8d424fd",
   "metadata": {},
   "source": [
    "Now that we have added **2000** more epochs to the training (for a total of **5000** epochs), let's check the predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c653dc-54ed-4c1a-aca0-419e2311f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4b82e-6635-4dcd-b889-8374ecb93dad",
   "metadata": {},
   "source": [
    "...and they look good!!! The prediction for **Company A** is super close to **0**, which is exactly what we want, and the prediction for **Company B** is close to **1**, which is also what we want. \n",
    "\n",
    "Now let's look at the graphs in **TensorBoard** by reloading that page.\n",
    "\n",
    "<img src=\"./images/train_loss_5000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_5000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_5000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "The dark red lines show how things changed when we added an additional **2000** epochs to the training, for a total of **5000** epochs. Now we see that the **loss** (`train_loss`) and the predictions for each company apper to be tapering off, suggesting that adding more epochs may not improve the predictions much, so we're done!\n",
    "\n",
    "Lastly, let's print out the final estimates for the Weights and Biases. In theory, they should be the same (within rounding error) as what we used in the **StatQuest** on **Long Short-Term Memory** and seen in the diagram of the **LSTM** unit at the top of this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4e6b4-7f55-4946-8e76-7a52cf2a39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1dbfc-ba9b-4eb3-9906-71a0edebc43d",
   "metadata": {},
   "source": [
    "## DOUBLE BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff877386-0b56-4c60-bcd1-bfa123b9fe7f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be30faf-c262-448a-b60d-8455ff9af997",
   "metadata": {},
   "source": [
    "<a id=\"nnLSTM\"></a>\n",
    "# Using and optimzing the PyTorch LSTM, nn.LSTM()\n",
    "\n",
    "Now that we know how to create an LSTM unit by hand, train it, and then use it to make good predictions, let's learn how to take advantage of PyTorch's `nn.LSTM()` function. For the most part, using `nn.LSTM()` allows us to simplify the `__init__()` function and the `forward()` function. The other big difference is that this time, we're not going to try and recreate the parameter values we used in the **StatQuest** on **Long Short-Term Memory**, and that means we can set the learning rate for the Adam to **0.1**. This will speed up training a lot. Everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781c6a1-035f-4149-b509-bf808855fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "         \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector\n",
    "        input_trans = input.view(len(input), 1)\n",
    "        \n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5899c78-1e93-45f3-96a5-c17654f507cc",
   "metadata": {},
   "source": [
    "Now let's create the model and print out the initial Weights and Biases and predictinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbb364-34a2-4896-a788-bbd4f8621ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "    \n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1cf65-09ca-4e94-b629-e606c6030953",
   "metadata": {},
   "source": [
    "As expected, the predictions are bad, so we will train the model. However, because we've increased the learning rate to **0.1**, we only need to train for **300** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c3792-9b36-4dd0-aeb2-dfbf96dc931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.\n",
    "## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    "## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,\n",
    "## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)\n",
    "## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So\n",
    "trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfd472-5e19-410f-adbe-06ff89ac2efd",
   "metadata": {},
   "source": [
    "Now that training is done, let's print out the new predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fa0f2-a6a1-4806-a3e7-0d670d7f30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41c31b-a69f-49bb-8faf-77df15a9e4f3",
   "metadata": {},
   "source": [
    "...and, as we can see, after just **300** epochs, the LSTM is making great predictions. The prediction for **Company A** is close to the observed value **0** and the prediction for **Company B** is close to the observed value **1**.\n",
    "\n",
    "Lastly, let's refresh the **TensorBoard** page to see the latest graphs. **NOTE:** To make it easier to see what we just did, deselect `version_0`, `version_1` and `version_2` and make sure `version_3` is checked on the left-hand side of the page, under where it says `Runs`. See below. This allows us to just look at the log files from the most rescent training, which only went for **300** epochs.\n",
    "\n",
    "<img src=\"./images/selecting_run_version_3.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n",
    "<img src=\"./images/train_loss_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_0_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_1_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f184fa-befe-4c9e-a7f5-e718ab84ea6d",
   "metadata": {},
   "source": [
    "In all three graphs, the loss (`train_loss`) and the predictions for **Company A** (`out_0`) and **Company B** (`out_1`) started to taper off after **500** steps, or just **250** epochs, suggesting that adding more epochs may not improve the predictions much, so we're done!\n",
    "\n",
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
